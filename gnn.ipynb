{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "from torch_geometric.utils import negative_sampling\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4161/3794757535.py:56: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.81682288646698, Accuracy: 0.4473684210526316\n",
      "Epoch 10, Loss: 0.7875545620918274, Accuracy: 0.9868421052631579\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 143\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m):\n\u001b[0;32m--> 143\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     acc \u001b[38;5;241m=\u001b[39m test()\n\u001b[1;32m    145\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "Cell \u001b[0;32mIn[7], line 122\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m out \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index)\n\u001b[1;32m    121\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out[data\u001b[38;5;241m.\u001b[39mtrain_mask], data\u001b[38;5;241m.\u001b[39my[data\u001b[38;5;241m.\u001b[39mtrain_mask])\n\u001b[0;32m--> 122\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    124\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = '/home/arifai/hussain/Race_pathways_signatures_meta_cleaned.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Remove columns with repeated names\n",
    "data = data.loc[:, ~data.columns.duplicated()]\n",
    "\n",
    "# Assign binary labels based on race\n",
    "data['label'] = data['Race'].apply(lambda x: 0 if x == \"White (Non-Hispanic)\" else 1 if x == \"African American\" else np.nan)\n",
    "\n",
    "# Drop rows with NaN labels (if any)\n",
    "data = data.dropna(subset=['label'])\n",
    "\n",
    "# Columns to remove\n",
    "columns_to_remove = [\n",
    "    'pathgs', 'pathgs_p', 'pathgs_s', 'Race', 'svi', 'sm', 'lni', 'epe', 'avg.risk', 'Decipher', 'APF', 'age',\n",
    "    'hallmark_adipogenesis', 'hallmark_allograft_rejection', 'hallmark_androgen_response', 'hallmark_angiogenesis',\n",
    "    'hallmark_angiogenesis_Brauer2013', 'hallmark_angiogenesis_KeggVEGF', 'hallmark_angiogenesis_Liberzon2015',\n",
    "    'hallmark_angiogenesis_Masiero2013', 'hallmark_angiogenesis_Nolan2013', 'hallmark_angiogenesis_Uhlik2016',\n",
    "    'hallmark_apical_junction', 'hallmark_apical_surface', 'hallmark_apoptosis', 'hallmark_bile_acid_metabolism',\n",
    "    'hallmark_cholesterol_homeostasis', 'hallmark_coagulation', 'hallmark_complement', 'hallmark_dna_repair',\n",
    "    'hallmark_e2f_targets', 'hallmark_epithelial_mesenchymal_transition', 'hallmark_estrogen_response_early',\n",
    "    'hallmark_estrogen_response_late', 'hallmark_fatty_acid_metabolism', 'hallmark_g2m_checkpoint', 'hallmark_glycolysis',\n",
    "    'hallmark_hedgehog_signaling', 'hallmark_heme_metabolism', 'hallmark_hypoxia', 'hallmark_il2_stat5_signaling',\n",
    "    'hallmark_il6_jak_stat3_signaling', 'hallmark_inflammatory_response', 'hallmark_interferon_alpha_response',\n",
    "    'hallmark_interferon_gamma_response', 'hallmark_kras_signaling_dn', 'hallmark_kras_signaling_up', 'hallmark_mitotic_spindle',\n",
    "    'hallmark_mtorc1_signaling', 'hallmark_myc_targets_v1', 'hallmark_myc_targets_v2', 'hallmark_myogenesis',\n",
    "    'hallmark_notch_signaling', 'hallmark_oxidative_phosphorylation', 'hallmark_p53_pathway', 'hallmark_pancreas_beta_cells',\n",
    "    'hallmark_peroxisome', 'hallmark_pi3k_akt_mtor_signaling', 'hallmark_protein_secretion', 'hallmark_reactive_oxigen_species_pathway',\n",
    "    'hallmark_spermatogenesis', 'hallmark_tgf_beta_signaling', 'hallmark_tnfa_signaling_via_nfkb', 'hallmark_unfolded_protein_response',\n",
    "    'hallmark_uv_response_dn', 'hallmark_uv_response_up', 'hallmark_wnt_beta_catenin_signaling', 'hallmark_xenobiotic_metabolism',\n",
    "    'agell2012_1', 'cheville2008_1', 'cuzick2011_1', 'decipher_1', 'glinsky2005_1', 'klein2014_1', 'lapointe2004_1', 'larkin2012_1',\n",
    "    'long2014_1', 'nakagawa2008_1', 'penney2011_1', 'ramaswamy2003_1', 'saal2007_1', 'singh2002_1', 'stephenson2005_1', 'talantov2010_1',\n",
    "    'varambally2005_1', 'wu2013_1', 'yu2007_1'\n",
    "]\n",
    "\n",
    "# Remove specified columns\n",
    "data = data.drop(columns=columns_to_remove)\n",
    "\n",
    "# Drop non-numeric columns (assuming the first 3 columns are metadata)\n",
    "gene_expression = data.iloc[:, 3:-1]  # Exclude the label column\n",
    "\n",
    "# Handle missing values (e.g., fill with the mean of the column)\n",
    "gene_expression = gene_expression.apply(pd.to_numeric, errors='coerce')\n",
    "gene_expression = gene_expression.fillna(gene_expression.mean())\n",
    "\n",
    "# Convert to numpy array\n",
    "gene_expression = gene_expression.values\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = np.corrcoef(gene_expression, rowvar=False)\n",
    "\n",
    "# Define a threshold for creating edges\n",
    "threshold = 0.7\n",
    "edges = np.where(np.abs(correlation_matrix) > threshold)\n",
    "edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "\n",
    "# Create node features (gene expression levels)\n",
    "x = torch.tensor(gene_expression, dtype=torch.float)\n",
    "\n",
    "# Use actual labels from the dataset\n",
    "y = torch.tensor(data['label'].values, dtype=torch.long)\n",
    "\n",
    "# Create PyG data object\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "# Manually split the data into training and test sets\n",
    "num_train = 1000\n",
    "num_test = 152\n",
    "\n",
    "# Create masks for training and test sets\n",
    "train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "\n",
    "train_mask[:num_train] = True\n",
    "test_mask[num_train:num_train + num_test] = True\n",
    "\n",
    "data.train_mask = train_mask\n",
    "data.test_mask = test_mask\n",
    "\n",
    "# Define GAT model for node classification\n",
    "class GATNodeClassification(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels, num_heads=8):\n",
    "        super(GATNodeClassification, self).__init__()\n",
    "        self.conv1 = GATConv(num_node_features, hidden_channels, heads=num_heads, dropout=0.6)\n",
    "        self.conv2 = GATConv(hidden_channels * num_heads, hidden_channels, heads=num_heads, dropout=0.6)\n",
    "        self.conv3 = GATConv(hidden_channels * num_heads, hidden_channels, heads=num_heads, dropout=0.6)\n",
    "        self.conv4 = GATConv(hidden_channels * num_heads, hidden_channels, heads=num_heads, dropout=0.6)\n",
    "        self.fc = torch.nn.Linear(hidden_channels * num_heads, 2)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = GATNodeClassification(num_node_features=data.num_features, hidden_channels=256)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "# Training loop for node classification\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Evaluation for node classification\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "        acc = int(correct) / int(data.test_mask.sum())\n",
    "        return acc\n",
    "\n",
    "# Store loss and accuracy for plotting\n",
    "train_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(200):\n",
    "    loss = train()\n",
    "    acc = test()\n",
    "    train_losses.append(loss)\n",
    "    test_accuracies.append(acc)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss}, Accuracy: {acc}')\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), '/root/gat_node_classification_model.pth')\n",
    "\n",
    "# Load the model\n",
    "model = GATNodeClassification(num_node_features=data.num_features, hidden_channels=256)\n",
    "model.load_state_dict(torch.load('/root/gat_node_classification_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Print the final test accuracy\n",
    "final_test_accuracy = test()\n",
    "print(f'Final Test Accuracy: {final_test_accuracy}')\n",
    "\n",
    "# Plotting the results\n",
    "epochs = range(200)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Training Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Test Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Test Accuracy over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/root/node_classification_loss_accuracy.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch_geometric\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "# Use the preprocessed data from Section 1\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = np.corrcoef(gene_expression, rowvar=False)\n",
    "\n",
    "print(np.shape(correlation_matrix))\n",
    "\n",
    "# Define a threshold for creating edges\n",
    "threshold = 0.7\n",
    "edges = np.where((np.abs(correlation_matrix) > threshold) & (np.abs(correlation_matrix != 1)))\n",
    "edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "\n",
    "print(np.shape(edges))\n",
    "\n",
    "# Create a NetworkX graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes (genes)\n",
    "num_genes = correlation_matrix.shape[0]\n",
    "G.add_nodes_from(range(num_genes))\n",
    "\n",
    "# Add edges based on the correlation threshold\n",
    "for i in range(edges[0].shape[0]):\n",
    "    G.add_edge(edges[0][i], edges[1][i])\n",
    "\n",
    "# Plot the graph\n",
    "plt.figure(figsize=(12, 12))\n",
    "pos = nx.spring_layout(G, seed=42)  # Position nodes using the spring layout\n",
    "nx.draw(G, pos, with_labels=True, node_size=50, node_color='skyblue', edge_color='gray', font_size=8)\n",
    "plt.title('Gene Interaction Network')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import community as community_louvain\n",
    "import torch_geometric\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/root/Race_pathways_signatures_meta_cleaned.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Remove columns with repeated names\n",
    "df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "# Assign binary labels based on race\n",
    "df['label'] = df['Race'].apply(lambda x: 0 if x == \"White (Non-Hispanic)\" else 1 if x == \"African American\" else np.nan)\n",
    "\n",
    "# Drop rows with NaN labels (if any)\n",
    "df = df.dropna(subset=['label'])\n",
    "\n",
    "# Columns to remove\n",
    "columns_to_remove = [\n",
    "    'pathgs', 'pathgs_p', 'pathgs_s', 'Race', 'svi', 'sm', 'lni', 'epe', 'avg.risk', 'Decipher', 'APF', 'age',\n",
    "    'hallmark_adipogenesis', 'hallmark_allograft_rejection', 'hallmark_androgen_response', 'hallmark_angiogenesis',\n",
    "    'hallmark_angiogenesis_Brauer2013', 'hallmark_angiogenesis_KeggVEGF', 'hallmark_angiogenesis_Liberzon2015',\n",
    "    'hallmark_angiogenesis_Masiero2013', 'hallmark_angiogenesis_Nolan2013', 'hallmark_angiogenesis_Uhlik2016',\n",
    "    'hallmark_apical_junction', 'hallmark_apical_surface', 'hallmark_apoptosis', 'hallmark_bile_acid_metabolism',\n",
    "    'hallmark_cholesterol_homeostasis', 'hallmark_coagulation', 'hallmark_complement', 'hallmark_dna_repair',\n",
    "    'hallmark_e2f_targets', 'hallmark_epithelial_mesenchymal_transition', 'hallmark_estrogen_response_early',\n",
    "    'hallmark_estrogen_response_late', 'hallmark_fatty_acid_metabolism', 'hallmark_g2m_checkpoint', 'hallmark_glycolysis',\n",
    "    'hallmark_hedgehog_signaling', 'hallmark_heme_metabolism', 'hallmark_hypoxia', 'hallmark_il2_stat5_signaling',\n",
    "    'hallmark_il6_jak_stat3_signaling', 'hallmark_inflammatory_response', 'hallmark_interferon_alpha_response',\n",
    "    'hallmark_interferon_gamma_response', 'hallmark_kras_signaling_dn', 'hallmark_kras_signaling_up', 'hallmark_mitotic_spindle',\n",
    "    'hallmark_mtorc1_signaling', 'hallmark_myc_targets_v1', 'hallmark_myc_targets_v2', 'hallmark_myogenesis',\n",
    "    'hallmark_notch_signaling', 'hallmark_oxidative_phosphorylation', 'hallmark_p53_pathway', 'hallmark_pancreas_beta_cells',\n",
    "    'hallmark_peroxisome', 'hallmark_pi3k_akt_mtor_signaling', 'hallmark_protein_secretion', 'hallmark_reactive_oxigen_species_pathway',\n",
    "    'hallmark_spermatogenesis', 'hallmark_tgf_beta_signaling', 'hallmark_tnfa_signaling_via_nfkb', 'hallmark_unfolded_protein_response',\n",
    "    'hallmark_uv_response_dn', 'hallmark_uv_response_up', 'hallmark_wnt_beta_catenin_signaling', 'hallmark_xenobiotic_metabolism',\n",
    "    'agell2012_1', 'cheville2008_1', 'cuzick2011_1', 'decipher_1', 'glinsky2005_1', 'klein2014_1', 'lapointe2004_1', 'larkin2012_1',\n",
    "    'long2014_1', 'nakagawa2008_1', 'penney2011_1', 'ramaswamy2003_1', 'saal2007_1', 'singh2002_1', 'stephenson2005_1', 'talantov2010_1',\n",
    "    'varambally2005_1', 'wu2013_1', 'yu2007_1'\n",
    "]\n",
    "\n",
    "# Remove specified columns\n",
    "df = df.drop(columns=columns_to_remove)\n",
    "\n",
    "# Extract gene names from the original DataFrame\n",
    "gene_names = df.columns[3:-1]\n",
    "\n",
    "# Drop non-numeric columns (assuming the first 3 columns are metadata)\n",
    "gene_expression = df.iloc[:, 3:-1].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values in gene expression data and update labels\n",
    "gene_expression = gene_expression.dropna()\n",
    "labels = df['label'].loc[gene_expression.index].values\n",
    "\n",
    "# Function to get top interactions\n",
    "def get_top_interactions(gene_expression, labels, race_label, top_n=5):\n",
    "    race_indices = np.where(labels == race_label)[0]\n",
    "    race_corr_matrix = np.corrcoef(gene_expression.iloc[race_indices].values, rowvar=False)\n",
    "    np.fill_diagonal(race_corr_matrix, 0)\n",
    "    strongest_interactions = np.unravel_index(np.argsort(-np.abs(race_corr_matrix), axis=None), race_corr_matrix.shape)\n",
    "    top_interactions = []\n",
    "    seen_pairs = set()\n",
    "    for i in range(len(strongest_interactions[0])):\n",
    "        gene1, gene2 = strongest_interactions[0][i], strongest_interactions[1][i]\n",
    "        if (gene1, gene2) not in seen_pairs and (gene2, gene1) not in seen_pairs and gene1 < len(gene_names) and gene2 < len(gene_names) and gene_names[gene1] != gene_names[gene2]:\n",
    "            top_interactions.append((gene1, gene2))\n",
    "            seen_pairs.add((gene1, gene2))\n",
    "            if len(top_interactions) == top_n:\n",
    "                break\n",
    "    return top_interactions\n",
    "\n",
    "# Function to visualize the correlation matrix\n",
    "def visualize_correlation_matrix(correlation_matrix, title):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(correlation_matrix, cmap='coolwarm', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(ticks=np.arange(len(gene_names)), labels=gene_names, rotation=90, fontsize=3)\n",
    "    plt.yticks(ticks=np.arange(len(gene_names)), labels=gene_names, fontsize=3)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Function to perform topological analysis\n",
    "def topological_analysis(gene_expression, labels, race_label, race_name):\n",
    "    race_indices = np.where(labels == race_label)[0]\n",
    "    race_corr_matrix = np.corrcoef(gene_expression.iloc[race_indices].values, rowvar=False)\n",
    "    np.fill_diagonal(race_corr_matrix, 0)\n",
    "    \n",
    "    # Construct the gene co-expression network\n",
    "    G = nx.Graph()\n",
    "    for gene in gene_names:\n",
    "        G.add_node(gene)\n",
    "    for i, gene1 in enumerate(gene_names):\n",
    "        for j, gene2 in enumerate(gene_names):\n",
    "            if i < j and abs(race_corr_matrix[i, j]) > 0.7:\n",
    "                G.add_edge(gene1, gene2, weight=race_corr_matrix[i, j])\n",
    "    \n",
    "    # Degree centrality\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    print(\"Degree Centrality:\", degree_centrality)\n",
    "    \n",
    "    # Betweenness centrality\n",
    "    betweenness_centrality = nx.betweenness_centrality(G)\n",
    "    print(\"Betweenness Centrality:\", betweenness_centrality)\n",
    "    \n",
    "    # Closeness centrality\n",
    "    closeness_centrality = nx.closeness_centrality(G)\n",
    "    print(\"Closeness Centrality:\", closeness_centrality)\n",
    "    \n",
    "    # Eigenvector centrality\n",
    "    eigenvector_centrality = nx.eigenvector_centrality(G)\n",
    "    print(\"Eigenvector Centrality:\", eigenvector_centrality)\n",
    "    \n",
    "    # Clustering coefficient\n",
    "    clustering_coefficient = nx.clustering(G)\n",
    "    print(\"Clustering Coefficient:\", clustering_coefficient)\n",
    "    \n",
    "    # Community detection using the Louvain method\n",
    "    partition = community_louvain.best_partition(G)\n",
    "    print(\"Community Detection (Louvain):\", partition)\n",
    "    \n",
    "    # Visualize the network with community detection\n",
    "    pos = nx.spring_layout(G)\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    nx.draw_networkx_nodes(G, pos, partition.keys(), node_size=40, cmap=cmap, node_color=list(partition.values()))\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "    plt.title(f\"Gene Co-expression Network for {race_name}\")\n",
    "    plt.show()\n",
    "\n",
    "# Get top 5 interactions for African Americans (label 1)\n",
    "top_interactions_aa = get_top_interactions(gene_expression, labels, 1)\n",
    "print(\"Top 5 strongest interactions for African Americans:\")\n",
    "for i, (gene1, gene2) in enumerate(top_interactions_aa):\n",
    "    print(f\"Interaction {i+1}: {gene_names[gene1]} - {gene_names[gene2]}\")\n",
    "\n",
    "# Visualize the correlation matrix for African Americans\n",
    "race_indices_aa = np.where(labels == 1)[0]\n",
    "race_corr_matrix_aa = np.corrcoef(gene_expression.iloc[race_indices_aa].values, rowvar=False)\n",
    "visualize_correlation_matrix(race_corr_matrix_aa, \"Correlation Matrix for African Americans\")\n",
    "\n",
    "# Perform topological analysis for African Americans\n",
    "topological_analysis(gene_expression, labels, 1, \"African Americans\")\n",
    "\n",
    "# Get top 5 interactions for White (Non-Hispanic) (label 0)\n",
    "top_interactions_wh = get_top_interactions(gene_expression, labels, 0)\n",
    "print(\"Top 5 strongest interactions for White (Non-Hispanic):\")\n",
    "for i, (gene1, gene2) in enumerate(top_interactions_wh):\n",
    "    print(f\"Interaction {i+1}: {gene_names[gene1]} - {gene_names[gene2]}\")\n",
    "\n",
    "# Visualize the correlation matrix for White (Non-Hispanic)\n",
    "race_indices_wh = np.where(labels == 0)[0]\n",
    "race_corr_matrix_wh = np.corrcoef(gene_expression.iloc[race_indices_wh].values, rowvar=False)\n",
    "visualize_correlation_matrix(race_corr_matrix_wh, \"Correlation Matrix for White (Non-Hispanic)\")\n",
    "\n",
    "# Perform topological analysis for White (Non-Hispanic)\n",
    "topological_analysis(gene_expression, labels, 0, \"White (Non-Hispanic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# GCN NODE CLASSIFICATION AND PREDICTION SECTION\n",
    "########################### (loss: the loss of current interations)\n",
    "############################# (accuracy: rate of properly classifying race per node)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/root/Race_pathways_signatures_meta_cleaned.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Remove columns with repeated names\n",
    "data = data.loc[:, ~data.columns.duplicated()]\n",
    "\n",
    "# Assign binary labels based on race\n",
    "data['label'] = data['Race'].apply(lambda x: 0 if x == \"White (Non-Hispanic)\" else 1 if x == \"African American\" else np.nan)\n",
    "\n",
    "# Drop rows with NaN labels (if any)\n",
    "data = data.dropna(subset=['label'])\n",
    "\n",
    "# Columns to remove\n",
    "columns_to_remove = [\n",
    "    'pathgs', 'pathgs_p', 'pathgs_s', 'Race', 'svi', 'sm', 'lni', 'epe', 'avg.risk', 'Decipher', 'APF', 'age',\n",
    "    'hallmark_adipogenesis', 'hallmark_allograft_rejection', 'hallmark_androgen_response', 'hallmark_angiogenesis',\n",
    "    'hallmark_angiogenesis_Brauer2013', 'hallmark_angiogenesis_KeggVEGF', 'hallmark_angiogenesis_Liberzon2015',\n",
    "    'hallmark_angiogenesis_Masiero2013', 'hallmark_angiogenesis_Nolan2013', 'hallmark_angiogenesis_Uhlik2016',\n",
    "    'hallmark_apical_junction', 'hallmark_apical_surface', 'hallmark_apoptosis', 'hallmark_bile_acid_metabolism',\n",
    "    'hallmark_cholesterol_homeostasis', 'hallmark_coagulation', 'hallmark_complement', 'hallmark_dna_repair',\n",
    "    'hallmark_e2f_targets', 'hallmark_epithelial_mesenchymal_transition', 'hallmark_estrogen_response_early',\n",
    "    'hallmark_estrogen_response_late', 'hallmark_fatty_acid_metabolism', 'hallmark_g2m_checkpoint', 'hallmark_glycolysis',\n",
    "    'hallmark_hedgehog_signaling', 'hallmark_heme_metabolism', 'hallmark_hypoxia', 'hallmark_il2_stat5_signaling',\n",
    "    'hallmark_il6_jak_stat3_signaling', 'hallmark_inflammatory_response', 'hallmark_interferon_alpha_response',\n",
    "    'hallmark_interferon_gamma_response', 'hallmark_kras_signaling_dn', 'hallmark_kras_signaling_up', 'hallmark_mitotic_spindle',\n",
    "    'hallmark_mtorc1_signaling', 'hallmark_myc_targets_v1', 'hallmark_myc_targets_v2', 'hallmark_myogenesis',\n",
    "    'hallmark_notch_signaling', 'hallmark_oxidative_phosphorylation', 'hallmark_p53_pathway', 'hallmark_pancreas_beta_cells',\n",
    "    'hallmark_peroxisome', 'hallmark_pi3k_akt_mtor_signaling', 'hallmark_protein_secretion', 'hallmark_reactive_oxigen_species_pathway',\n",
    "    'hallmark_spermatogenesis', 'hallmark_tgf_beta_signaling', 'hallmark_tnfa_signaling_via_nfkb', 'hallmark_unfolded_protein_response',\n",
    "    'hallmark_uv_response_dn', 'hallmark_uv_response_up', 'hallmark_wnt_beta_catenin_signaling', 'hallmark_xenobiotic_metabolism',\n",
    "    'agell2012_1', 'cheville2008_1', 'cuzick2011_1', 'decipher_1', 'glinsky2005_1', 'klein2014_1', 'lapointe2004_1', 'larkin2012_1',\n",
    "    'long2014_1', 'nakagawa2008_1', 'penney2011_1', 'ramaswamy2003_1', 'saal2007_1', 'singh2002_1', 'stephenson2005_1', 'talantov2010_1',\n",
    "    'varambally2005_1', 'wu2013_1', 'yu2007_1'\n",
    "]\n",
    "\n",
    "# Remove specified columns\n",
    "data = data.drop(columns=columns_to_remove)\n",
    "\n",
    "# Drop non-numeric columns (assuming the first 3 columns are metadata)\n",
    "gene_expression = data.iloc[:, 3:-1]  # Exclude the label column\n",
    "\n",
    "# Handle missing values (e.g., fill with the mean of the column)\n",
    "gene_expression = gene_expression.apply(pd.to_numeric, errors='coerce')\n",
    "gene_expression = gene_expression.fillna(gene_expression.mean())\n",
    "\n",
    "# Convert to numpy array\n",
    "gene_expression = gene_expression.values\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = np.corrcoef(gene_expression, rowvar=False)\n",
    "\n",
    "# Define a threshold for creating edges\n",
    "threshold = 0.7\n",
    "edges = np.where(np.abs(correlation_matrix) > threshold)\n",
    "edges = np.array(edges)  # Convert to a single numpy array\n",
    "edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "\n",
    "# Create node features (gene expression levels)\n",
    "x = torch.tensor(gene_expression, dtype=torch.float)\n",
    "\n",
    "# Use actual labels from the dataset\n",
    "y = torch.tensor(data['label'].values, dtype=torch.long)\n",
    "\n",
    "# Create PyG data object\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "# Manually split the data into training and test sets\n",
    "num_train = 1000\n",
    "num_test = 152\n",
    "\n",
    "# Create masks for training and test sets\n",
    "train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "\n",
    "train_mask[:num_train] = True\n",
    "test_mask[num_train:num_train + num_test] = True\n",
    "\n",
    "data.train_mask = train_mask\n",
    "data.test_mask = test_mask\n",
    "\n",
    "# Define GNN model for node classification\n",
    "class GCNNodeClassification(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels):\n",
    "        super(GCNNodeClassification, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv4 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn4 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.fc = torch.nn.Linear(hidden_channels, 2)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = GCNNodeClassification(num_node_features=data.num_features, hidden_channels=256)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "# Training loop for node classification\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Evaluation for node classification\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "        acc = int(correct) / int(data.test_mask.sum())\n",
    "        return acc\n",
    "\n",
    "# Store loss and accuracy for plotting\n",
    "train_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(200):\n",
    "    loss = train()\n",
    "    acc = test()\n",
    "    train_losses.append(loss)\n",
    "    test_accuracies.append(acc)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss}, Accuracy: {acc}')\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), '/root/gcn_node_classification_model.pth')\n",
    "\n",
    "# Load the model\n",
    "model = GCNNodeClassification(num_node_features=data.num_features, hidden_channels=256)\n",
    "model.load_state_dict(torch.load('/root/gcn_node_classification_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Print the final test accuracy\n",
    "final_test_accuracy = test()\n",
    "print(f'Final Test Accuracy: {final_test_accuracy}')\n",
    "\n",
    "# Plotting the results for node classification\n",
    "epochs = range(200)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Training Loss for Node Classification\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, label='Training Loss for Node Classification')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss for Node Classification over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Test Accuracy for Node Classification\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, test_accuracies, label='Test Accuracy for Node Classification')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Test Accuracy for Node Classification over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/root/node_classification_loss_accuracy.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/root/Race_pathways_signatures_meta_cleaned.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Remove columns with repeated names\n",
    "data = data.loc[:, ~data.columns.duplicated()]\n",
    "\n",
    "# Assign binary labels based on race\n",
    "data['label'] = data['Race'].apply(lambda x: 0 if x == \"White (Non-Hispanic)\" else 1 if x == \"African American\" else np.nan)\n",
    "\n",
    "# Drop rows with NaN labels (if any)\n",
    "data = data.dropna(subset=['label'])\n",
    "\n",
    "# Columns to remove\n",
    "columns_to_remove = [\n",
    "    'pathgs', 'pathgs_p', 'pathgs_s', 'Race', 'svi', 'sm', 'lni', 'epe', 'avg.risk', 'Decipher', 'APF', 'age',\n",
    "    'hallmark_adipogenesis', 'hallmark_allograft_rejection', 'hallmark_androgen_response', 'hallmark_angiogenesis',\n",
    "    'hallmark_angiogenesis_Brauer2013', 'hallmark_angiogenesis_KeggVEGF', 'hallmark_angiogenesis_Liberzon2015',\n",
    "    'hallmark_angiogenesis_Masiero2013', 'hallmark_angiogenesis_Nolan2013', 'hallmark_angiogenesis_Uhlik2016',\n",
    "    'hallmark_apical_junction', 'hallmark_apical_surface', 'hallmark_apoptosis', 'hallmark_bile_acid_metabolism',\n",
    "    'hallmark_cholesterol_homeostasis', 'hallmark_coagulation', 'hallmark_complement', 'hallmark_dna_repair',\n",
    "    'hallmark_e2f_targets', 'hallmark_epithelial_mesenchymal_transition', 'hallmark_estrogen_response_early',\n",
    "    'hallmark_estrogen_response_late', 'hallmark_fatty_acid_metabolism', 'hallmark_g2m_checkpoint', 'hallmark_glycolysis',\n",
    "    'hallmark_hedgehog_signaling', 'hallmark_heme_metabolism', 'hallmark_hypoxia', 'hallmark_il2_stat5_signaling',\n",
    "    'hallmark_il6_jak_stat3_signaling', 'hallmark_inflammatory_response', 'hallmark_interferon_alpha_response',\n",
    "    'hallmark_interferon_gamma_response', 'hallmark_kras_signaling_dn', 'hallmark_kras_signaling_up', 'hallmark_mitotic_spindle',\n",
    "    'hallmark_mtorc1_signaling', 'hallmark_myc_targets_v1', 'hallmark_myc_targets_v2', 'hallmark_myogenesis',\n",
    "    'hallmark_notch_signaling', 'hallmark_oxidative_phosphorylation', 'hallmark_p53_pathway', 'hallmark_pancreas_beta_cells',\n",
    "    'hallmark_peroxisome', 'hallmark_pi3k_akt_mtor_signaling', 'hallmark_protein_secretion', 'hallmark_reactive_oxigen_species_pathway',\n",
    "    'hallmark_spermatogenesis', 'hallmark_tgf_beta_signaling', 'hallmark_tnfa_signaling_via_nfkb', 'hallmark_unfolded_protein_response',\n",
    "    'hallmark_uv_response_dn', 'hallmark_uv_response_up', 'hallmark_wnt_beta_catenin_signaling', 'hallmark_xenobiotic_metabolism',\n",
    "    'agell2012_1', 'cheville2008_1', 'cuzick2011_1', 'decipher_1', 'glinsky2005_1', 'klein2014_1', 'lapointe2004_1', 'larkin2012_1',\n",
    "    'long2014_1', 'nakagawa2008_1', 'penney2011_1', 'ramaswamy2003_1', 'saal2007_1', 'singh2002_1', 'stephenson2005_1', 'talantov2010_1',\n",
    "    'varambally2005_1', 'wu2013_1', 'yu2007_1'\n",
    "]\n",
    "\n",
    "# Remove specified columns\n",
    "data = data.drop(columns=columns_to_remove)\n",
    "\n",
    "# Drop non-numeric columns (assuming the first 3 columns are metadata)\n",
    "gene_expression = data.iloc[:, 3:-1]  # Exclude the label column\n",
    "\n",
    "# Handle missing values (e.g., fill with the mean of the column)\n",
    "gene_expression = gene_expression.apply(pd.to_numeric, errors='coerce')\n",
    "gene_expression = gene_expression.fillna(gene_expression.mean())\n",
    "\n",
    "# Convert to numpy array\n",
    "gene_expression = gene_expression.values\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = np.corrcoef(gene_expression, rowvar=False)\n",
    "\n",
    "# Define a threshold for creating edges\n",
    "threshold = 0.7\n",
    "edges = np.where(np.abs(correlation_matrix) > threshold)\n",
    "\n",
    "# Convert the list of numpy arrays to a single numpy array\n",
    "edges = np.array(edges)\n",
    "\n",
    "# Create edge_index tensor\n",
    "edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "\n",
    "# Create node features (gene expression levels)\n",
    "x = torch.tensor(gene_expression, dtype=torch.float)\n",
    "\n",
    "# Use actual labels from the dataset\n",
    "y = torch.tensor(data['label'].values, dtype=torch.long)\n",
    "\n",
    "# Create PyG data object\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "# Generate negative samples for link prediction\n",
    "negative_edge_index = negative_sampling(edge_index, num_neg_samples=edge_index.size(1))\n",
    "\n",
    "# Combine positive and negative edges for training\n",
    "train_edge_index = torch.cat([edge_index, negative_edge_index], dim=1)\n",
    "\n",
    "# Create edge labels (1 for positive edges, 0 for negative edges)\n",
    "train_edge_labels = torch.cat([torch.ones(edge_index.size(1)), torch.zeros(negative_edge_index.size(1))], dim=0).unsqueeze(1)\n",
    "\n",
    "# Define GNN model for node classification\n",
    "class GCNNodeClassification(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels):\n",
    "        super(GCNNodeClassification, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv4 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn4 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.fc = torch.nn.Linear(hidden_channels, 2)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# Define MLP model for link prediction\n",
    "class MLPLinkPrediction(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels):\n",
    "        super(MLPLinkPrediction, self).__init__()\n",
    "        self.gcn = GCNNodeClassification(num_node_features, hidden_channels)\n",
    "        self.fc1 = torch.nn.Linear(hidden_channels * 2, hidden_channels)\n",
    "        self.fc2 = torch.nn.Linear(hidden_channels, 1)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_index_pos, edge_index_neg):\n",
    "        x = self.gcn(x, edge_index)\n",
    "        pos_edge_embeddings = torch.cat([x[edge_index_pos[0]], x[edge_index_pos[1]]], dim=1)\n",
    "        neg_edge_embeddings = torch.cat([x[edge_index_neg[0]], x[edge_index_neg[1]]], dim=1)\n",
    "        edge_embeddings = torch.cat([pos_edge_embeddings, neg_edge_embeddings], dim=0)\n",
    "        x = F.relu(self.fc1(edge_embeddings))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss, and optimizer for link prediction\n",
    "model_link_pred = MLPLinkPrediction(num_node_features=data.num_features, hidden_channels=256)\n",
    "criterion_link_pred = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer_link_pred = torch.optim.Adam(model_link_pred.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop for link prediction\n",
    "def train_link_prediction():\n",
    "    model_link_pred.train()\n",
    "    optimizer_link_pred.zero_grad()\n",
    "    out = model_link_pred(data.x, data.edge_index, edge_index, negative_edge_index)\n",
    "    loss = criterion_link_pred(out, train_edge_labels)\n",
    "    loss.backward()\n",
    "    optimizer_link_pred.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Evaluation for link prediction\n",
    "def test_link_prediction():\n",
    "    model_link_pred.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model_link_pred(data.x, data.edge_index, edge_index, negative_edge_index)\n",
    "        pred = torch.sigmoid(out)\n",
    "        pred_labels = (pred > 0.5).float()\n",
    "        correct = (pred_labels == train_edge_labels).sum()\n",
    "        acc = int(correct) / len(train_edge_labels)\n",
    "        return acc\n",
    "\n",
    "# Store loss and accuracy for plotting\n",
    "train_losses_link_pred = []\n",
    "test_accuracies_link_pred = []\n",
    "\n",
    "# Train the link prediction model\n",
    "for epoch in range(200):\n",
    "    loss = train_link_prediction()\n",
    "    acc = test_link_prediction()\n",
    "    train_losses_link_pred.append(loss)\n",
    "    test_accuracies_link_pred.append(acc)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Link Prediction Loss: {loss}, Link Prediction Accuracy: {acc}')\n",
    "\n",
    "# Save the link prediction model\n",
    "torch.save(model_link_pred.state_dict(), '/root/mlp_link_prediction_model.pth')\n",
    "\n",
    "# Load the link prediction model\n",
    "model_link_pred = MLPLinkPrediction(num_node_features=data.num_features, hidden_channels=256)\n",
    "model_link_pred.load_state_dict(torch.load('/root/mlp_link_prediction_model.pth'))\n",
    "model_link_pred.eval()\n",
    "\n",
    "# Print the final test accuracy for link prediction\n",
    "final_test_accuracy_link_pred = test_link_prediction()\n",
    "print(f'Final Test Accuracy for Link Prediction: {final_test_accuracy_link_pred}')\n",
    "\n",
    "# Plotting the results for link prediction\n",
    "epochs = range(200)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Training Loss for Link Prediction\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses_link_pred, label='Training Loss for Link Prediction')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss for Link Prediction over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Test Accuracy for Link Prediction\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, test_accuracies_link_pred, label='Test Accuracy for Link Prediction')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Test Accuracy for Link Prediction over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/root/link_prediction_loss_accuracy.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
